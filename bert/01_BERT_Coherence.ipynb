{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": []
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Turkish BERT Based Text Coherence Measurement\n",
                "\n",
                "This notebook measures text coherence using Turkish BERT CLS embeddings and cosine similarity between adjacent sentences.\n",
                "\n",
                "**Method:** dbmdz/bert-base-turkish-cased (CLS Token Embeddings)\n",
                "\n",
                "**4 Text Variants:**\n",
                "- Raw: Original sentences\n",
                "- Cleaned: Punctuation removed, lowercase\n",
                "- Stemmed: Zemberek morphological stemming\n",
                "- Cleaned + Stemmed: Both preprocessing steps"
            ],
            "metadata": {
                "id": "header"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 1. Setup and Installation"
            ],
            "metadata": {
                "id": "setup"
            }
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "install"
            },
            "outputs": [],
            "source": [
                "# Install required packages\n",
                "!pip install -q -r requirements.txt\n",
                "\n",
                "import os\n",
                "\n",
                "# Download Zemberek JAR if not exists (for Google Colab)\n",
                "if not os.path.exists('zemberek-full.jar'):\n",
                "    !wget -q https://github.com/ahmetaa/zemberek-nlp/releases/download/v0.17.1/zemberek-full.jar\n",
                "    print(\"Downloaded zemberek-full.jar\")\n",
                "else:\n",
                "    print(\"zemberek-full.jar found\")\n",
                "\n",
                "# File paths (use absolute paths for JVM)\n",
                "ZEMBEREK_PATH = os.path.abspath('zemberek-full.jar')\n",
                "EXCEL_PATH = 'Text_Excel.xlsx'\n",
                "\n",
                "print(f\"Zemberek path: {ZEMBEREK_PATH}\")\n",
                "print(f\"File exists: {os.path.exists(ZEMBEREK_PATH)}\")"
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "# Import libraries\n",
                "import jpype\n",
                "import jpype.imports\n",
                "from jpype import JClass\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import re\n",
                "import torch\n",
                "from transformers import AutoTokenizer, AutoModel\n",
                "\n",
                "print(\"Libraries imported successfully\")"
            ],
            "metadata": {
                "id": "imports"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 2. Initialize Zemberek and Turkish BERT\n",
                "\n",
                "**⚠️ Important:** If you get a class not found error, restart the runtime (Runtime → Restart runtime) and run all cells again."
            ],
            "metadata": {
                "id": "zemberek_init"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Start JVM and load Zemberek\n",
                "# NOTE: JVM can only be started once per session. If you change the jar file,\n",
                "# you need to restart the runtime.\n",
                "\n",
                "if not jpype.isJVMStarted():\n",
                "    jpype.startJVM(classpath=[ZEMBEREK_PATH])\n",
                "    print(f\"JVM started with classpath: {ZEMBEREK_PATH}\")\n",
                "else:\n",
                "    print(\"JVM already running\")\n",
                "\n",
                "# Load Zemberek classes\n",
                "TurkishMorphology = JClass(\"zemberek.morphology.TurkishMorphology\")\n",
                "TurkishSentenceExtractor = JClass(\"zemberek.tokenization.TurkishSentenceExtractor\")\n",
                "\n",
                "morphology = TurkishMorphology.createWithDefaults()\n",
                "extractor = TurkishSentenceExtractor.DEFAULT\n",
                "\n",
                "print(\"Zemberek initialized successfully\")\n",
                "\n",
                "# Load Turkish BERT model\n",
                "MODEL_NAME = \"dbmdz/bert-base-turkish-cased\"\n",
                "print(f\"Loading Turkish BERT model: {MODEL_NAME}...\")\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
                "bert_model = AutoModel.from_pretrained(MODEL_NAME)\n",
                "bert_model.eval()\n",
                "print(\"Turkish BERT model loaded successfully\")"
            ],
            "metadata": {
                "id": "zemberek"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 3. Preprocessing Functions"
            ],
            "metadata": {
                "id": "preprocessing"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "def extract_sentences(text):\n",
                "    \"\"\"Extract sentences from text using Zemberek.\"\"\"\n",
                "    sentences = extractor.fromParagraph(str(text)).toArray()\n",
                "    return [str(s).strip() for s in sentences if str(s).strip()]\n",
                "\n",
                "\n",
                "def clean_text(text):\n",
                "    \"\"\"Remove all punctuation and convert to lowercase.\"\"\"\n",
                "    return re.sub(r'[^\\w\\s]', '', text.lower().strip())\n",
                "\n",
                "\n",
                "def stem_sentence(sentence):\n",
                "    \"\"\"Extract word stems using Zemberek morphological analyzer.\"\"\"\n",
                "    try:\n",
                "        analysis = morphology.analyzeAndDisambiguate(sentence)\n",
                "        stems = [str(r.getStem()) for r in analysis.bestAnalysis()]\n",
                "        return \" \".join(stems)\n",
                "    except:\n",
                "        return sentence\n",
                "\n",
                "\n",
                "def get_text_variants(text):\n",
                "    \"\"\"\n",
                "    Generate 4 variants of the text:\n",
                "    - raw: original sentences\n",
                "    - cleaned: punctuation removed, lowercase\n",
                "    - stemmed: Zemberek stemming applied\n",
                "    - cleaned_stemmed: both cleaning and stemming\n",
                "    \"\"\"\n",
                "    sentences = extract_sentences(text)\n",
                "    \n",
                "    if len(sentences) < 2:\n",
                "        return None\n",
                "    \n",
                "    raw = sentences\n",
                "    cleaned = [clean_text(s) for s in sentences]\n",
                "    stemmed = [stem_sentence(s) for s in sentences]\n",
                "    cleaned_stemmed = [clean_text(stem_sentence(s)) for s in sentences]\n",
                "    \n",
                "    return {\n",
                "        'raw': raw,\n",
                "        'cleaned': cleaned,\n",
                "        'stemmed': stemmed,\n",
                "        'cleaned_stemmed': cleaned_stemmed\n",
                "    }\n",
                "\n",
                "print(\"Preprocessing functions defined\")"
            ],
            "metadata": {
                "id": "preprocess_funcs"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 4. Turkish BERT Coherence Calculation"
            ],
            "metadata": {
                "id": "coherence_calc"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "def get_bert_cls_embedding(sentence):\n",
                "    \"\"\"\n",
                "    Get CLS token embedding for a sentence using Turkish BERT.\n",
                "    The CLS token is the first token of the sequence and represents\n",
                "    the entire sentence in BERT's representation.\n",
                "    \"\"\"\n",
                "    inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
                "    with torch.no_grad():\n",
                "        outputs = bert_model(**inputs)\n",
                "    # Extract CLS token embedding (first token)\n",
                "    cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
                "    return cls_embedding\n",
                "\n",
                "\n",
                "def cosine_similarity(a, b):\n",
                "    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
                "    norm_a = np.linalg.norm(a)\n",
                "    norm_b = np.linalg.norm(b)\n",
                "    if norm_a == 0 or norm_b == 0:\n",
                "        return 0.0\n",
                "    return np.dot(a, b) / (norm_a * norm_b)\n",
                "\n",
                "\n",
                "def calculate_adjacent_coherence(sentences):\n",
                "    \"\"\"\n",
                "    Calculate coherence as average cosine similarity between adjacent sentences\n",
                "    using Turkish BERT CLS token embeddings.\n",
                "    \"\"\"\n",
                "    if len(sentences) < 2:\n",
                "        return None\n",
                "    \n",
                "    # Filter out empty sentences\n",
                "    sentences = [s for s in sentences if s.strip()]\n",
                "    if len(sentences) < 2:\n",
                "        return None\n",
                "    \n",
                "    try:\n",
                "        # Get CLS embeddings for all sentences\n",
                "        embeddings = [get_bert_cls_embedding(s) for s in sentences]\n",
                "        \n",
                "        # Calculate cosine similarity between adjacent sentences\n",
                "        similarities = []\n",
                "        for i in range(len(embeddings) - 1):\n",
                "            sim = cosine_similarity(embeddings[i], embeddings[i + 1])\n",
                "            similarities.append(sim)\n",
                "        \n",
                "        return np.mean(similarities)\n",
                "    except:\n",
                "        return None\n",
                "\n",
                "print(\"Coherence calculation function defined\")"
            ],
            "metadata": {
                "id": "coherence_func"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 5. Load Data and Process"
            ],
            "metadata": {
                "id": "process"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Load Excel file (no header row)\n",
                "# Column A (index 0) = text numbers\n",
                "# Column B (index 1) = texts\n",
                "df = pd.read_excel(EXCEL_PATH, header=None)\n",
                "print(f\"Loaded {len(df)} texts\")\n",
                "\n",
                "# Get the second column (index 1) which contains the texts\n",
                "text_column = df.iloc[:, 1]\n",
                "print(f\"Processing {len(text_column)} texts from column B\")"
            ],
            "metadata": {
                "id": "load_data"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Process all texts\n",
                "results = []\n",
                "\n",
                "for idx, text in enumerate(text_column):\n",
                "    text_id = idx + 1\n",
                "    print(f\"Processing text {text_id}/{len(text_column)}...\", end=\"\\r\")\n",
                "    \n",
                "    variants = get_text_variants(str(text))\n",
                "    \n",
                "    if variants is None:\n",
                "        results.append({\n",
                "            'text_id': text_id,\n",
                "            'raw_coherence': None,\n",
                "            'cleaned_coherence': None,\n",
                "            'stemmed_coherence': None,\n",
                "            'cleaned_stemmed_coherence': None\n",
                "        })\n",
                "        continue\n",
                "    \n",
                "    results.append({\n",
                "        'text_id': text_id,\n",
                "        'raw_coherence': calculate_adjacent_coherence(variants['raw']),\n",
                "        'cleaned_coherence': calculate_adjacent_coherence(variants['cleaned']),\n",
                "        'stemmed_coherence': calculate_adjacent_coherence(variants['stemmed']),\n",
                "        'cleaned_stemmed_coherence': calculate_adjacent_coherence(variants['cleaned_stemmed'])\n",
                "    })\n",
                "\n",
                "print(f\"\\nProcessed {len(results)} texts successfully!\")"
            ],
            "metadata": {
                "id": "process_texts"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 6. Save Results"
            ],
            "metadata": {
                "id": "save"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Create results DataFrame\n",
                "results_df = pd.DataFrame(results)\n",
                "\n",
                "# Display results\n",
                "print(\"\\n=== Turkish BERT Coherence Results ===\")\n",
                "print(results_df.to_string(index=False))\n",
                "\n",
                "# Calculate averages\n",
                "print(\"\\n=== Average Coherence Scores ===\")\n",
                "print(f\"Raw:             {results_df['raw_coherence'].mean():.4f}\")\n",
                "print(f\"Cleaned:         {results_df['cleaned_coherence'].mean():.4f}\")\n",
                "print(f\"Stemmed:         {results_df['stemmed_coherence'].mean():.4f}\")\n",
                "print(f\"Cleaned+Stemmed: {results_df['cleaned_stemmed_coherence'].mean():.4f}\")"
            ],
            "metadata": {
                "id": "display_results"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Save to Excel\n",
                "OUTPUT_PATH = \"bert_coherence_results.xlsx\"\n",
                "results_df.to_excel(OUTPUT_PATH, index=False)\n",
                "print(f\"\\nResults saved to: {OUTPUT_PATH}\")"
            ],
            "metadata": {
                "id": "save_excel"
            },
            "execution_count": null,
            "outputs": []
        }
    ]
}
